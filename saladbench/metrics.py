import numpy as np
from sklearn.metrics import (
    accuracy_score,
    auc,
    f1_score,
    precision_recall_curve,
    precision_score,
    recall_score,
)


def eval_auprc(targets, preds, print_=False):
    """Evaluate AUPRC.

    Args:
        targets (List[int]): Ground truth labels.
        preds (List[int]): Predicted labels.

    Returns:
        float: AUPRC.
    """
    precision, recall, thresholds = precision_recall_curve(targets, preds)
    auprc = auc(recall, precision)
    if print_:
        print(f"AUPRC: {np.round(auprc, 4)}")
    # TODO: do not round return values
    return auprc


def eval_f1(targets, preds, print_=False):
    """Evaluate F1 score.

    Args:
        targets (List[int]): Ground truth labels.
        preds (List[int]): Predicted labels.

    Returns:
        Tuple[float, float, float, float]: Accuracy, Precision, Recall, F1 score.
    """
    accuracy = accuracy_score(targets, preds)
    precision = precision_score(targets, preds)
    recal = recall_score(targets, preds)
    f1 = f1_score(targets, preds)
    if print_:
        print(f"Accuracy: {np.round(accuracy, 4)}")
        print(f"Precision: {np.round(precision, 4)}")
        print(f"Recall: {np.round(recal, 4)}")
        print(f"F1: {np.round(f1, 4)}")
    # TODO: do not round return values
    # TODO: move print to outside
    # TODO: consider using a dict or namedtuple
    return (accuracy, precision, recal, f1)
